{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec619e2d",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "**Simple Linear Regression**\n",
    "- In this there is only one independent feature and one dependent feature is available\n",
    "\n",
    "Ex.\n",
    "We have dataset like Height,Weight\n",
    "And Height is independent feature whereas weight is a dependent feature\n",
    "from height we have to predict weight\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "- In this there is multiple independent feature and one dependent feature is available\n",
    "\n",
    "Ex.\n",
    "We have dataset like number of rooms , size if the house , number of bath , price of the house\n",
    "In this set number of rooms , size if the house , number of bath are independent feature and price of the house is a dependent feature\n",
    "from the independent feature we have to predict price of the house"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279dc34",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "- Linearity: The relationship between the independent variables (features) and the dependent variable (target) is assumed to be linear. This means that the relationship can be represented by a straight line in a scatterplot.\n",
    "\n",
    "- Independence: The observations in the dataset are assumed to be independent of each other. There should be no correlation or dependence between the residuals (the differences between the observed and predicted values).\n",
    "\n",
    "- Homoscedasticity: The variability of the residuals should be constant across all levels of the independent variables. In other words, the spread of the residuals should be the same across the range of predicted values.\n",
    "\n",
    "- Normality: The residuals are assumed to be normally distributed. This assumption allows for valid statistical inference, such as hypothesis testing and confidence interval estimation.\n",
    "\n",
    "- No multicollinearity: The independent variables should not be highly correlated with each other. Multicollinearity can lead to unstable parameter estimates and make it difficult to interpret the individual effects of the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1fcf0c",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "- Intercept: The intercept represents the predicted value of the dependent variable when all independent variables are set to zero.\n",
    "- Slope: Each slope coefficient represents the change in the dependent variable associated with a one-unit change in the corresponding independent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08003cbd",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Gradient descent is an optimization algorithm commonly used in machine learning and other numerical optimization problems. Its goal is to minimize a given objective function by iteratively adjusting the parameters of a model.\n",
    "\n",
    "Gradient descent aims to find the global minimum of the objective function, but it may only reach a local minimum depending on the function's properties and the starting point. There are variations of gradient descent, such as stochastic gradient descent (SGD) and mini-batch gradient descent, which introduce randomness or process subsets of the data to speed up convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce35cb9",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "- In this there is multiple independent feature and one dependent feature is available\n",
    "\n",
    "Ex.\n",
    "We have dataset like number of rooms , size if the house , number of bath , price of the house\n",
    "In this set number of rooms , size if the house , number of bath are independent feature and price of the house is a dependent feature\n",
    "from the independent feature we have to predict price of the house\n",
    "\n",
    "- It differ from simple liner regression based on the dependent feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8866715",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "Multicollinearity refers to a high degree of correlation between two or more independent variables in a multiple linear regression model. It occurs when independent variables are linearly dependent or highly correlated, which can pose challenges in interpreting the model and estimating the individual effects of the variables. Multicollinearity does not affect the relationship between the independent variables and the dependent variable but can lead to unstable or unreliable coefficient estimates.\n",
    "\n",
    "**Detecting Multicollinearity:**\n",
    "- Correlation Matrix: Calculate the correlation coefficients between pairs of independent variables. Correlation values close to +1 or -1 indicate high correlation.\n",
    "\n",
    "- Variance Inflation Factor (VIF): Compute the VIF for each independent variable. The VIF measures the extent to which the variance of the estimated coefficient is inflated due to multicollinearity. VIF values greater than 5 or 10 are generally considered indicative of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e95cb",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Polynomial regression is a type of regression analysis that extends the linear regression model to capture non-linear relationships between the independent variables (features) and the dependent variable (target). While linear regression assumes a linear relationship, polynomial regression allows for curved relationships by including higher-order terms of the independent variables in the model.\n",
    "\n",
    "In polynomial regression, the model equation includes additional terms with higher powers of the independent variables\n",
    "\n",
    "The main difference between linear regression and polynomial regression is the linearity assumption. Linear regression assumes a linear relationship, while polynomial regression allows for non-linear relationships by incorporating higher-order terms in the model equation. By doing so, polynomial regression can better capture complex patterns and curvatures in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4996a",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "**Advantages**\n",
    "- Captures Non-Linear Relationships: Polynomial regression allows for capturing non-linear relationships between the independent variables and the dependent variable. It can better represent curved patterns in the data that cannot be captured by linear regression.\n",
    "\n",
    "- Flexibility in Model Complexity: By adjusting the degree of the polynomial, polynomial regression provides flexibility in modeling various degrees of complexity. Higher-degree polynomials can capture more intricate relationships, allowing for a better fit to the data.\n",
    "    \n",
    "**Disadvantages**\n",
    "- Overfitting: Polynomial regression, especially with high degrees of polynomials, is prone to overfitting. Overfitting occurs when the model fits the training data too closely but fails to generalize well to new, unseen data. Regularization techniques like ridge regression or cross-validation can be employed to mitigate overfitting.\n",
    "\n",
    "- Increased Model Complexity: As the degree of the polynomial increases, the model becomes more complex, which can make it challenging to interpret and explain the relationships between variables. It may also increase the computational complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f30167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
