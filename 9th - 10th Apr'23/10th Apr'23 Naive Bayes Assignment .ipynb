{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a5d6cb",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "The probability that an employee is a smoker given that he/she uses the health insurance plan is 0.70 or 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945b9a6",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle and the underlying assumptions about the features.\n",
    "\n",
    "**Bernoulli Naive Bayes:**\n",
    "\n",
    "- Applicability: Bernoulli Naive Bayes is suited for binary feature data, where each feature can take on one of two values (usually 0 or 1).\n",
    "- Assumption: It assumes that each feature is conditionally independent of other features given the class label.\n",
    "- Data Representation: It works with \"presence/absence\" data, where each feature indicates whether a particular word or feature is present (1) or absent (0) in a document or sample.\n",
    "- Example: Bernoulli Naive Bayes is often used in text classification tasks where the features represent the occurrence of specific words in a document.\n",
    "\n",
    "**Multinomial Naive Bayes:**\n",
    "\n",
    "- Applicability: Multinomial Naive Bayes is suitable for discrete feature data, where each feature represents a count or frequency of occurrences (e.g., word frequency).\n",
    "- Assumption: Similar to Bernoulli Naive Bayes, it assumes that each feature is conditionally independent of other features given the class label.\n",
    "- Data Representation: It is used when the features are counts or frequencies (non-negative integers) of events in a sample or document.\n",
    "- Example: Multinomial Naive Bayes is commonly used in text classification tasks where the features represent the frequency of words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d71314",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values in a straightforward manner. When dealing with missing values in the input data, the Bernoulli Naive Bayes algorithm typically treats them as a separate category or class for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0955892",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that is suitable for continuous (real-valued) features that are assumed to follow a Gaussian (normal) distribution within each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfd6b1",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "**Data preparation:**\n",
    "    \n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "**Results:**\n",
    "    \n",
    "Report the following performance metrics for each classifier:\\\n",
    "Accuracy\\\n",
    "Precision\\\n",
    "Recall\\\n",
    "F1 score\\\n",
    "    \n",
    "**Discussion:**\n",
    "        \n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "**Conclusion:**\n",
    "    \n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "**Note: Create your assignment in Jupyter notebook and upload it to GitHub & share that github repository\n",
    "link through your dashboard. Make sure the repository is public.\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad689f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3     4     5     6     7     8     9   ...     48  \\\n",
       "0     0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "1     0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.000   \n",
       "2     0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.010   \n",
       "3     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "4     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "...    ...   ...   ...  ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "4596  0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4597  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4598  0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.102   \n",
       "4599  0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4600  0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "         49   50     51     52     53     54   55    56  57  \n",
       "0     0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1     0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2     0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3     0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4     0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "...     ...  ...    ...    ...    ...    ...  ...   ...  ..  \n",
       "4596  0.232  0.0  0.000  0.000  0.000  1.142    3    88   0  \n",
       "4597  0.000  0.0  0.353  0.000  0.000  1.555    4    14   0  \n",
       "4598  0.718  0.0  0.000  0.000  0.000  1.404    6   118   0  \n",
       "4599  0.057  0.0  0.000  0.000  0.000  1.147    5    78   0  \n",
       "4600  0.000  0.0  0.125  0.000  0.000  1.250    5    40   0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(r\"D:\\Study\\Data Science\\Datasets\\spambase.data\", header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9254ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ff92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_clf = BernoulliNB()\n",
    "multinomial_clf = MultinomialNB()\n",
    "gaussian_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3921d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(bernoulli_clf, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276b27ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fold 1: 0.8850325379609545\n",
      "Accuracy of fold 2: 0.9173913043478261\n",
      "Accuracy of fold 3: 0.9\n",
      "Accuracy of fold 4: 0.908695652173913\n",
      "Accuracy of fold 5: 0.8913043478260869\n",
      "Accuracy of fold 6: 0.9282608695652174\n",
      "Accuracy of fold 7: 0.9260869565217391\n",
      "Accuracy of fold 8: 0.8913043478260869\n",
      "Accuracy of fold 9: 0.808695652173913\n",
      "Accuracy of fold 10: 0.782608695652174\n",
      "Average Accuracy across all folds: 0.8839380364047911\n",
      "---------------------------------------------------------\n",
      "Precision of fold 1: 0.8975326966098778\n",
      "Precision of fold 2: 0.9176356589147286\n",
      "Precision of fold 3: 0.9137634408602151\n",
      "Precision of fold 4: 0.9114535182331793\n",
      "Precision of fold 5: 0.892552645095018\n",
      "Precision of fold 6: 0.9259502749223045\n",
      "Precision of fold 7: 0.9316890789283427\n",
      "Precision of fold 8: 0.8989952406134321\n",
      "Precision of fold 9: 0.7994194484760523\n",
      "Precision of fold 10: 0.7721867321867322\n",
      "Average Precision across all folds: 0.8861178734839884\n",
      "---------------------------------------------------------\n",
      "Recall of fold 1: 0.8639469849147268\n",
      "Recall of fold 2: 0.9088860779508261\n",
      "Recall of fold 3: 0.8802672147995889\n",
      "Recall of fold 4: 0.8965920117230044\n",
      "Recall of fold 5: 0.8783738291847363\n",
      "Recall of fold 6: 0.9233945226638152\n",
      "Recall of fold 7: 0.9138398780173864\n",
      "Recall of fold 8: 0.8735222479653062\n",
      "Recall of fold 9: 0.8063922057862531\n",
      "Recall of fold 10: 0.774213350759421\n",
      "Average Recall across all folds: 0.8719428323765064\n",
      "---------------------------------------------------------\n",
      "F1 Score of fold 1: 0.8398791540785498\n",
      "F1 Score of fold 2: 0.8926553672316384\n",
      "F1 Score of fold 3: 0.8614457831325301\n",
      "F1 Score of fold 4: 0.8786127167630058\n",
      "F1 Score of fold 5: 0.8554913294797687\n",
      "F1 Score of fold 6: 0.9080779944289694\n",
      "F1 Score of fold 7: 0.9011627906976744\n",
      "F1 Score of fold 8: 0.851190476190476\n",
      "F1 Score of fold 9: 0.7659574468085106\n",
      "F1 Score of fold 10: 0.726775956284153\n",
      "Average F1 Score across all folds: 0.8481249015095276\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of fold {i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Average Precision across all folds: {np.mean(scores['test_precision_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")   \n",
    "print(f\"Average Recall across all folds: {np.mean(scores['test_recall_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 Score of fold {i+1}: {scores['test_f1'][i]}\") \n",
    "print(f\"Average F1 Score across all folds: {np.mean(scores['test_f1'])}\" )\n",
    "print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1bf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(gaussian_clf, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318d3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fold 1: 0.8438177874186551\n",
      "Accuracy of fold 2: 0.8630434782608696\n",
      "Accuracy of fold 3: 0.8782608695652174\n",
      "Accuracy of fold 4: 0.8673913043478261\n",
      "Accuracy of fold 5: 0.8847826086956522\n",
      "Accuracy of fold 6: 0.8282608695652174\n",
      "Accuracy of fold 7: 0.8326086956521739\n",
      "Accuracy of fold 8: 0.8673913043478261\n",
      "Accuracy of fold 9: 0.6347826086956522\n",
      "Accuracy of fold 10: 0.717391304347826\n",
      "Average Accuracy across all folds: 0.8217730830896915\n",
      "---------------------------------------------------------\n",
      "Precision of fold 1: 0.8487462292609351\n",
      "Precision of fold 2: 0.8665588162948668\n",
      "Precision of fold 3: 0.8782608695652174\n",
      "Precision of fold 4: 0.8704222154963681\n",
      "Precision of fold 5: 0.8792075070283414\n",
      "Precision of fold 6: 0.8447074142156863\n",
      "Precision of fold 7: 0.8430462568472307\n",
      "Precision of fold 8: 0.8634469696969698\n",
      "Precision of fold 9: 0.7185854544618426\n",
      "Precision of fold 10: 0.7424239475271999\n",
      "Average Precision across all folds: 0.8355405680394657\n",
      "---------------------------------------------------------\n",
      "Recall of fold 1: 0.8642817755720982\n",
      "Recall of fold 2: 0.88289588109732\n",
      "Recall of fold 3: 0.8954858091548739\n",
      "Recall of fold 4: 0.8877700548525713\n",
      "Recall of fold 5: 0.8953147587080932\n",
      "Recall of fold 6: 0.8564823065803283\n",
      "Recall of fold 7: 0.8571555872393513\n",
      "Recall of fold 8: 0.8800075249014832\n",
      "Recall of fold 9: 0.6882512525000495\n",
      "Recall of fold 10: 0.7466484484841284\n",
      "Average Recall across all folds: 0.8454293399090297\n",
      "---------------------------------------------------------\n",
      "F1 Score of fold 1: 0.8293838862559241\n",
      "F1 Score of fold 2: 0.8496420047732697\n",
      "F1 Score of fold 3: 0.8640776699029125\n",
      "F1 Score of fold 4: 0.8537170263788968\n",
      "F1 Score of fold 5: 0.8658227848101266\n",
      "F1 Score of fold 6: 0.8192219679633868\n",
      "F1 Score of fold 7: 0.8205128205128206\n",
      "F1 Score of fold 8: 0.8478802992518704\n",
      "F1 Score of fold 9: 0.6692913385826772\n",
      "F1 Score of fold 10: 0.711111111111111\n",
      "Average F1 Score across all folds: 0.8130660909542995\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of fold {i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Average Precision across all folds: {np.mean(scores['test_precision_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")   \n",
    "print(f\"Average Recall across all folds: {np.mean(scores['test_recall_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 Score of fold {i+1}: {scores['test_f1'][i]}\") \n",
    "print(f\"Average F1 Score across all folds: {np.mean(scores['test_f1'])}\" )\n",
    "print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02f741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "scoring = ['precision_macro', 'recall_macro','accuracy','f1']\n",
    "scores = cross_validate(multinomial_clf, X, y, scoring=scoring, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7276b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fold 1: 0.7917570498915402\n",
      "Accuracy of fold 2: 0.7934782608695652\n",
      "Accuracy of fold 3: 0.808695652173913\n",
      "Accuracy of fold 4: 0.8347826086956521\n",
      "Accuracy of fold 5: 0.8282608695652174\n",
      "Accuracy of fold 6: 0.7782608695652173\n",
      "Accuracy of fold 7: 0.7782608695652173\n",
      "Accuracy of fold 8: 0.8130434782608695\n",
      "Accuracy of fold 9: 0.6934782608695652\n",
      "Accuracy of fold 10: 0.7434782608695653\n",
      "Average Accuracy across all folds: 0.7863496180326323\n",
      "---------------------------------------------------------\n",
      "Precision of fold 1: 0.7846028180518685\n",
      "Precision of fold 2: 0.7887858083119164\n",
      "Precision of fold 3: 0.8025521852576647\n",
      "Precision of fold 4: 0.8276337066538899\n",
      "Precision of fold 5: 0.8230185909980431\n",
      "Precision of fold 6: 0.7678085051392671\n",
      "Precision of fold 7: 0.7676658476658477\n",
      "Precision of fold 8: 0.8081634339303051\n",
      "Precision of fold 9: 0.6963907384987893\n",
      "Precision of fold 10: 0.7314987714987715\n",
      "Average Precision across all folds: 0.7798120406006364\n",
      "---------------------------------------------------------\n",
      "Recall of fold 1: 0.7744692583402261\n",
      "Recall of fold 2: 0.7731638864732391\n",
      "Recall of fold 3: 0.7933433473001819\n",
      "Recall of fold 4: 0.8249866333986812\n",
      "Recall of fold 5: 0.8137883918493436\n",
      "Recall of fold 6: 0.7667478563931959\n",
      "Recall of fold 7: 0.769658805124854\n",
      "Recall of fold 8: 0.7954216915186438\n",
      "Recall of fold 9: 0.7055882294698905\n",
      "Recall of fold 10: 0.7332224400483178\n",
      "Average Recall across all folds: 0.7750390539916573\n",
      "---------------------------------------------------------\n",
      "F1 Score of fold 1: 0.7241379310344828\n",
      "F1 Score of fold 2: 0.7214076246334311\n",
      "F1 Score of fold 3: 0.7485714285714287\n",
      "F1 Score of fold 4: 0.7877094972067039\n",
      "F1 Score of fold 5: 0.7736389684813754\n",
      "F1 Score of fold 6: 0.7166666666666666\n",
      "F1 Score of fold 7: 0.7213114754098361\n",
      "F1 Score of fold 8: 0.75\n",
      "F1 Score of fold 9: 0.6618705035971223\n",
      "F1 Score of fold 10: 0.6775956284153005\n",
      "Average F1 Score across all folds: 0.7282909724016348\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#scores for each fold\n",
    "for i in range(0,10):\n",
    "    print(f\"Accuracy of fold {i+1}: {scores['test_accuracy'][i]}\")\n",
    "print(f\"Average Accuracy across all folds: {np.mean(scores['test_accuracy'])}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Precision of fold {i+1}: {scores['test_precision_macro'][i]}\")\n",
    "print(f\"Average Precision across all folds: {np.mean(scores['test_precision_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"Recall of fold {i+1}: {scores['test_recall_macro'][i]}\")   \n",
    "print(f\"Average Recall across all folds: {np.mean(scores['test_recall_macro'])}\" )\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"F1 Score of fold {i+1}: {scores['test_f1'][i]}\") \n",
    "print(f\"Average F1 Score across all folds: {np.mean(scores['test_f1'])}\" )\n",
    "print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e5d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78ce63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25633a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = bernoulli_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "522f2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       531\n",
      "           1       0.91      0.80      0.85       390\n",
      "\n",
      "    accuracy                           0.88       921\n",
      "   macro avg       0.89      0.87      0.88       921\n",
      "weighted avg       0.88      0.88      0.88       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluating\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f0628d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_preds, labels=bernoulli_clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=bernoulli_clf.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
